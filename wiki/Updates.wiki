#summary Updates from implementation

= Latest News =

I've made a lot of progress on the packet marshalling code, and I'm done converting the code to being completely non-blocking.  I'm pretty sure this code won't ever get out of sync with the server, now that it understands the header fields _much_ better.

The API only exists as scribbles on a notepad, but I'm thinking it will support two modes of operation:
  * Single-threaded: clients explicitly call a loop() method with a timeout parameter similar to asyncore.loop()
  * Multi-threaded: a single background thread dealing with the IO by wrapping asyncore.loop() with a thread, and messages are enqueued and dequeued from two thread safe deque objects by the client.  I'm not 100% sure on the details yet.

Note: I use the words "channel" and "group" interchangably.  When I write "channel" you can understand it to mean "Spread group", with no special meaning attached.  I personally think in terms of "channels" more than "groups".  Sorry for any confusion.

== Thoughts about the API ==

So far, the API looks pretty simple.  join(), leave(), disconnect(), multicast(), unicast(), and now ping(). The next big thing to add are either callback listeners, or queue based "listeners" (multi-thread safe).

This is where is gets tricky.  If the API and data-paths are too complicated, then simple use-cases will require a lot of plumbing, which would suck.  But I don't want to make too many assumptions about the behavior of the end-users of this API.  Of course, there are no end-users yet, just me, so I am trying not to impose my own particular needs on everyone else.

== The protocol ==

The protocol is both interesting and annoying.  It's interesting that every data message a client receives includes the list of groups that it was sent to, even if the client is not join'd onto those groups.  So there's a hidden "cost" when multicasting a message to multiple groups, if your group list is long, and clients only subscribe to a small subset of those groups.  The exact cost is 32 bytes per group, in the message header.  You could consider this to be a kind of "information" leakage, where a client can tell what extra groups a message was sent to.

At first, I didn't like this feature of spread.  But, actually- it opens up some really interesting functionality.  I can think of several cases where this may open up some very powerful features for distributed computing.  For example, a client could send a message to two destinations ('requests', '#worker003#srv001') as a multicast _and_ a unicast message.  This destination list shows up to _all_ recipients.  Very interesting.  Especially if you want to do replication or failover.

The other aspect of this, is that a message may be directed to multiple groups, and listeners on each group will know that a message has been "categorized" in some way.  For example, if there is a channel for "Events" and another channel for "Alerts", the same message may be directed to both channels when an Event is also an Alert.  A listener to just the "Events" channel would gain additional knowledge about this categorization just from the list of groups ["Events","Alerts"] that it sees on receipt of a new "Event" message.  Clearly, this could be abused all sorts of ways, so I'm not advocating using the destination group list as some crazy way of attaching metadata to every message.  But, in the legitimate cases where a message may be sent to multiple channels, this extra information may be quite useful.

On the annoying side of the protocol, the fact that the protocol doesn't align itself to a particular endian-ness is a pain.  I only have little-endian servers to test with, so I don't have proper endian correction code in place.  I'd rather not support it than write untestable code.  The message type (hint) field in particular is client-endianness, but the service type and length fields are all big-endian (network order).  It seems the goal of this endianness-preservation is to permit clients to send messages using their own endianness, and detect any endian-differences between others senders and themselves, for every single message, to permit an application to invoke different payload demarshalling code if it needs to do endian-swapping.  This logic for detecting client-client endian differences is also used on the service_type field, to detect client-server endian differences too!  Clearly the server isn't likely to change endianness from packet to packet.  If anything, it should be a property of the connection that is established at connection time for client-server endianness differences.  Then every field of the header would have a known endianness, and the "message type" field could be endian-corrected by the server, saving bytes.  I think it's more likely than not that a spread network would consist of primarily similar endianness, and flipping bytes is cheap anyhow.  A single bit in each message header could indicate client-client endian differences, so the payload could be processed with different decoders and all would be happy.

Also, the 32-byte group name length is deeply etched into the protocol.  While I don't mind the idea of fixed-width group and client names, it makes for challenges if you choose to compile with a different value.  In an ideal world, I think the server should _provide_ the value of MAXGROUPLEN as part of the initial connection negotiation.  Then every client would be compatible, no matter what the value of MAXGROUPLEN.  At present, this value is hard coded in every client implementation, and in the server.  Changing it on the server requires changing every client API.  That seems like work that could be easily avoided with a negotiated max group length parameter.

The fixed-width group names really could be more efficiently encoded.  There is a notion of "group ID" which isn't documented very well, that might suit the purpose.  The API and the user guide documentation both treat group IDs as opaque values that have no internal structure.  It feels like the server-server protocol is leaking into the server-client side here, since you don't really _need_ to process these group IDs at all.  The group ID seems to be 3 int32 values.  Those would be a lot smaller on the wire for clients, but I'm not really complaining.  The 32 bytes which also encodes the source spread daemon's name isn't too high a price to pay, and it significantly simplifies the client API.

The client-server protocol has no keep alive messages.  This is pretty cool.  It means a client's TCP connection to a server can survive an hours-long network outage when there is no traffic in either direction, and as long as TCP KEEPALIVE is not enabled.  The flip side of this is that there are no acknowledgements for regular data messages, so a client could be connected to a spread daemon which is hung, without realizing it.  This is a problem for every asynchronous or asymmetric protocol, so it just needs a way to do an application-level ping.  Well, that's easy (and implemented now) by sending a message that is addressed to one's self using the private name as the destination group name.  This is _exactly_ the functionality that is needed, and it's nice that the spread fills that need perfectly.
