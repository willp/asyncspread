#summary Updates from implementation

= Latest News =

=== May 5, 2010 ===

There's a lot to report this time.  The listener class(es) are working great, and the core protocol code is getting even tighter, plus I added a major bugfix and API change with join/leave, that only can handle one group at a time.  Here's the svn commit log which has more details:

big fix- join and leave only REALLY work for a single group at a time, i had to look in the server source (session.c) to see that the daemon really only looks at the first group given in a join or leave message.  that bug lingered for a long time. wow.

also, finished removing the ping timeout functionality and have a periodic call into the listener's _process_timer() method where the SpreadPingListener performs ping timeouts

added ping send/recv packet counters, so you can get "packet loss" stats.

i also realized that the "SEND" packet value was really the RELIABLE_MESS value, so I have a new (small) can of worms to fix- how to best handle changing the service_type (the reliability class) among: UNRELIABLE, RELIABLE, FIFO, CAUSAL, AGREED, SAFE... It needs to remain efficient, but also be simple and easy to modify.  I am leaning towards a .set_default_service(SpreadProto.RELIABLE_MESS) method, which calculates the packed int for that level, and for the "noreflect" version.

I'm also thinking that I need to add more debug statements to help application developers troubleshoot at the application level. this is still an idea in progress though.

I also hope to try out some multiple inheritance to mix together a ping listener with a debug listener... or provide some mechanism for easily adding debug behavior to any listener.

through continued refactoring, i was able to completely remove the st_read_memb_change() state, since it was identical, line for line, with the read message body method, now that membership changes are calculated entirely in the listener.

i also converted almost ALL debug output to logging module logger.debug() calls and added a nullhandler log object (as recommended by the logging module docs).

wired up the asyncore handle_error() method, so that it now calls the users (listener's) handle_error() method, passing the exception object in the same way the SpreadException object is sent!  This looks super clean now.  It even clears the exceptions there too. Neat. I expect I'll need to add a traceback object of some sort down the road...  Ehhh, later.

Added docs to constructor for AsyncSpread. Removed useless instance variables (group membership dict, start_time). Started to clean up the tra/ts test scripts.

and from the previous commit:

removed cb_dropped and cb_connect callbacks, instead am calling the listener's _process_\* methods, which then invoke local handle_\* methods

also, added new handle_error (and _process_error) which replaces the ill suited raise SpreadException() calls that were sprinkled throughout the handshake and connection processing code.  this is starting to look really clean now.

made first stab at properly doing logging using the logging module... this may be tricky for a little while, to make sure the library plays well as a library.

also i realized the SpreadException class desparately needs to be subclassed into categories, with perhaps an errcode -> exception class lookup map to make it easier to map spread errno to exception classes.  i think maybe 3 classes overall will be needed, connection level errors, protocol version/authentication errors, and others.  we'll see, it's not a high priority though.

the pinglistener code is still messy, and i've made no progress on filterchain style (MINA inspired) processing.  plus, the ping expiration part is just disabled for now, so i have to figure out a way to hook onto or implement a timer resource/callback from the poll/loop back to the listener, so it can spend time doing work if needed.

thread safety is another concern that weighs heavily.  i'll get there.



=== April 25, 2010 ===

Finally finished migrating the message processing to the new Factory approach.  The new code is so much cleaner, it's a relief.  The SpreadListener base class is finally usable, but I still have the "self-ping" functionality buried in the AsyncSpread class, which has to change.  I'm thrilled that some long-duration testing has proved the reliability of this code.  Various versions of the code have been running non-stop for weeks, without any failures or unhandled exceptions.  The TCP connection to the Spread daemon is super reliable.  No issues there.

I like seeing the debug output that shows off the new Message classes, like DataMessage.  Check it out:

{{{
!-!-!  SpreadListener:  Received message: <class 'asyncspread.DataMessage'>:  sender:#tra-000054#dev103,  mesg_type:65535,  groups:['#tra-000054#dev103'],  self-disc:False,  data:"PING:92644:1272210857.79026604"
GOT MESSAGE (mtype:0xffff) 93769 (30 bytes):  PING:92644:1272210857.79026604
Client PING callback: Success= True
   Elapsed time: 0.00193095
}}}

I'm slowly removing lots of debug output, but haven't yet switched the code over to using the logging module.  A higher priority is to fix up the main loop to be less stupid, and easier to work with for both non-threaded and threaded environments.

...

I'm now working on a new SpreadPingListener, which has me scratching my head because now the ping() method belongs in the SpreadPingListener class... It's definitely weird to have the ping() implemented in a **Listener class... Listeners shouldn't speak... I'm going to finish moving the ping() code out of the main AsyncSpread class, and then take a second look at where it best belongs.  The code is sprouting new classes all over the place, so I'm nervous about adding any more classes that only serve to be 'pure'.  Instead, I might look for a different adjective than "Listener" and call it something else, like Processor or Manager...  I'm thinking a lot.

=== April 8, 2010 ===

Got the message factory ironed out.  I'm still removing the old style message creation, though.  And I'm starting to move the callback mechanism out of the Asyncspread class, and into a SpreadListener class.  This will make it super easy to implement different message-dispatch mechanisms by subclassing SpreadListener, or by implementing its protocol ("Interface" to java folks).

I didn't really expect to go in this direction when I started, but it makes a lot of sense now that I've actually implemented a group registration process for callbacks into user code.  The asyncore/asynchat derived class gets so messy dealing with the message-routing specific logic, it ends up looking like spaghetti.  So I'm repeatedly refactoring.  One nice part of this refactoring is that the code is simplified, and easier to enhance.  After I discovered the "SELF_DISCARD" flag and tested it out, it was a piece of cake to add to the newly refactored DataMessage class without cluttering up any of the MembershipMessage classes.

Another cool thing, is that I learned you don't have to define a constructor for every class in python.  I don't know why, but I assumed they were necessary.  Well, not so, and now the various flavors of MembershipMessage subclasses have almost no code in them at all, except where it makes sense- like the extra boolean "self_leave" property of LeaveMessage objects, which only makes sense for that specific message.

I'm still wondering just where I might stick a deque in all this, to make it easier to run multi-threaded code.  Maybe in the SpreadListener?  I'm also wondering what the listener's API should be.  And wondering if the various Message classes should be consumable by the code, or if they should remain purely output objects.  At the moment, sending a message or performing a join/leave/disconnect operation is just a method call, not an object creation (other than strings).


=== April 7, 2010 ===

I'm again refactoring the code, to really clean up the message class.  The 'canonical' Java API for Spread has an extremely weak SpreadMessage class, and I realized that I was starting to mimic it for no good reason.  It is weak because a SpreadMessage object doesn't actually map to the _type_ of Spread message that it represents.  It is an unholy mess of boolean methods that you must query to determine what actual type of message you're dealing with.  And it also supports lots of methods that don't make any sense for a SpreadMessage of certain types.  For example, the getMembershipInfo() method exists for all SpreadMessage objects, but it is only valid when isMembership() returns True.  Otherwise, the result is undefined.

It looks like what's needed here is a tiny bit of class inheritance.  And I usually shy away from introducing layers of inheritance out of abject worship of simplicity, but here it just makes perfect sense.  There really are types of messages:  SpreadMessage -> (DataMessage, MembershipMessage, TransitionalMessage)  and MembershipMessage -> (JoinMessage, LeaveMessage, NetworkMessage).  It's not a deep hierarchy, and the basemost class will be pretty tiny.  I'm adding a MessageFactory class to deal with the appropriate creation of these objects from parsed spread messages (packets), which also seems to be a common use-case for a message factory pattern.  I am keeping a close eye on the efficiency, so that DataMessage objects are inexpensive.  This is where I think using python slots may help.

Once I get the MessageFactory ironed out and the individual classes wired up for the various message types, then I intend on refactoring the dispatch code that currently lives inside AsyncSpread's st_read_message() and st_read_groups() methods.  It's ridiculously overly complicated right now, so what I need to do is pull it apart a little bit, and let the code separate out into more uniform pieces.  Specifically, the AsyncSpread class will allow you to register a listener callback for various Spread message types, (DataMessage and MembershipMessage being the two big ones).  Those callbacks will then be able to deal with different message types without having to go through the rigamarole of testing isMembership(), isRegular(), causedByJoin(), causedByNetwork(), etc.  Instead, the methods can immediately implement the exact code needed to deal with that specific type of message.

Only at _this_ point is where it makes any sense to process callbacks for data messages, versus callbacks for membership-change messages.  It's not really rocket science, just moving some deeply nested and redundant code out of the st_read_message() and st_read_groups() methods that currently live in AsyncSpread.  This opens up a much cleaner API for implementing different types of message dispatch strategies.  For example, one client may need to map a received message to a specific handler based entirely on the group it was sent on.  Other clients may need to include the 16-bit message-type header value as part of the message call dispatching, to route a message to an application-specific handler method.  Yet another client might not care at all about which groups a message was sent to, or what the message-type is set to, so a single data message handler is all that is required.  Or suppose a client only needs to care about membership messages, and needs to discard all DataMessages.  This is where it makes a lot of sense to me, at least, to break apart the message->code routing into another layer.  I suspect I'll end up implementing a few different MessageDispatcher classes that implement convenient APIs for each of the use-cases I just described.

So, that's what's going on now.  I probably typed more characters describing the changes than the code will sum up to.  To sum up, for poor grammar's sake, a preposition is what I will end with.


== Recent News ==

=== March 31, 2010 ===

I've made a lot of progress on the packet marshalling code, and I'm done converting the code to being completely non-blocking.  I'm pretty sure this code won't ever get out of sync with the server, now that it understands the header fields _much_ better.

The API only exists as scribbles on a notepad, but I'm thinking it will support two modes of operation:
  * Single-threaded: clients explicitly call a loop() method with a timeout parameter similar to asyncore.loop()
  * Multi-threaded: a single background thread dealing with the IO by wrapping asyncore.loop() with a thread, and messages are enqueued and dequeued from two thread safe deque objects by the client.  I'm not 100% sure on the details yet.

Note: I use the words "channel" and "group" interchangably.  When I write "channel" you can understand it to mean "Spread group", with no special meaning attached.  I personally think in terms of "channels" more than "groups".  Sorry for any confusion.

== Thoughts about the API ==

So far, the API looks pretty simple.  join(), leave(), disconnect(), multicast(), unicast(), and now ping(). The next big thing to add are either callback listeners, or queue based "listeners" (multi-thread safe).

This is where is gets tricky.  If the API and data-paths are too complicated, then simple use-cases will require a lot of plumbing, which would suck.  But I don't want to make too many assumptions about the behavior of the end-users of this API.  Of course, there are no end-users yet, just me, so I am trying not to impose my own particular needs on everyone else.

== The protocol ==

The protocol is both interesting and annoying.  It's interesting that every data message a client receives includes the list of groups that it was sent to, even if the client is not join'd onto those groups.  So there's a hidden "cost" when multicasting a message to multiple groups, if your group list is long, and clients only subscribe to a small subset of those groups.  The exact cost is 32 bytes per group, in the message header.  You could consider this to be a kind of "information" leakage, where a client can tell what extra groups a message was sent to.

At first, I didn't like this feature of spread.  But, actually- it opens up some really interesting functionality.  I can think of several cases where this may open up some very powerful features for distributed computing.  For example, a client could send a message to two destinations ('requests', '#worker003#srv001') as a multicast _and_ a unicast message.  This destination list shows up to _all_ recipients.  Very interesting.  Especially if you want to do replication or failover.

The other aspect of this, is that a message may be directed to multiple groups, and listeners on each group will know that a message has been "categorized" in some way.  For example, if there is a channel for "Events" and another channel for "Alerts", the same message may be directed to both channels when an Event is also an Alert.  A listener to just the "Events" channel would gain additional knowledge about this categorization just from the list of groups ["Events","Alerts"] that it sees on receipt of a new "Event" message.  Clearly, this could be abused all sorts of ways, so I'm not advocating using the destination group list as some crazy way of attaching metadata to every message.  But, in the legitimate cases where a message may be sent to multiple channels, this extra information may be quite useful.

On the annoying side of the protocol, the fact that the protocol doesn't align itself to a particular endian-ness is a pain.  I only have little-endian servers to test with, so I don't have proper endian correction code in place.  I'd rather not support it than write untestable code.  The message type (hint) field in particular is client-endianness, but the service type and length fields are all big-endian (network order).  It seems the goal of this endianness-preservation is to permit clients to send messages using their own endianness, and detect any endian-differences between others senders and themselves, for every single message, to permit an application to invoke different payload demarshalling code if it needs to do endian-swapping.  This logic for detecting client-client endian differences is also used on the service_type field, to detect client-server endian differences too!  Clearly the server isn't likely to change endianness from packet to packet.  If anything, it should be a property of the connection that is established at connection time for client-server endianness differences.  Then every field of the header would have a known endianness, and the "message type" field could be endian-corrected by the server, saving bytes.  I think it's more likely than not that a spread network would consist of primarily similar endianness, and flipping bytes is cheap anyhow.  A single bit in each message header could indicate client-client endian differences, so the payload could be processed with different decoders and all would be happy.

Also, the 32-byte group name length is deeply etched into the protocol.  While I don't mind the idea of fixed-width group and client names, it makes for challenges if you choose to compile with a different value.  In an ideal world, I think the server should _provide_ the value of MAXGROUPLEN as part of the initial connection negotiation.  Then every client would be compatible, no matter what the value of MAXGROUPLEN.  At present, this value is hard coded in every client implementation, and in the server.  Changing it on the server requires changing every client API.  That seems like work that could be easily avoided with a negotiated max group length parameter.

The fixed-width group names really could be more efficiently encoded.  There is a notion of "group ID" which isn't documented very well, that might suit the purpose.  The API and the user guide documentation both treat group IDs as opaque values that have no internal structure.  It feels like the server-server protocol is leaking into the server-client side here, since you don't really _need_ to process these group IDs at all.  The group ID seems to be 3 int32 values.  Those would be a lot smaller on the wire for clients, but I'm not really complaining.  The 32 bytes which also encodes the source spread daemon's name isn't too high a price to pay, and it significantly simplifies the client API.

The client-server protocol has no keep alive messages.  This is pretty cool.  It means a client's TCP connection to a server can survive an hours-long network outage when there is no traffic in either direction, and as long as TCP KEEPALIVE is not enabled.  The flip side of this is that there are no acknowledgements for regular data messages, so a client could be connected to a spread daemon which is hung, without realizing it.  This is a problem for every asynchronous or asymmetric protocol, so it just needs a way to do an application-level ping.  Well, that's easy (and implemented now) by sending a message that is addressed to one's self using the private name as the destination group name.  This is _exactly_ the functionality that is needed, and it's nice that the spread fills that need perfectly.